# Comprehensive robots.txt to block all crawlers and bots
# Generated to maximize protection against scraping and AI training

# Default rule - block all bots from everything
User-agent: *
Disallow: /
Crawl-delay: 86400

# Major search engines
User-agent: Googlebot
Disallow: /

User-agent: Bingbot
Disallow: /

User-agent: Slurp
Disallow: /

User-agent: DuckDuckBot
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: YandexBot
Disallow: /

# Social media crawlers
User-agent: facebookexternalhit
Disallow: /

User-agent: Twitterbot
Disallow: /

User-agent: LinkedInBot
Disallow: /

User-agent: WhatsApp
Disallow: /

User-agent: TelegramBot
Disallow: /

# AI Training and LLM crawlers
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web/*
Disallow: /

User-agent: PerplexityBot
Disallow: /

User-agent: YouBot
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: ChatGPT
Disallow: /

User-agent: OpenAI
Disallow: /

User-agent: AI2Bot
Disallow: /

User-agent: Ai2Bot
Disallow: /

User-agent: Scrapy
Disallow: /

User-agent: cohere-ai
Disallow: /

User-agent: Meta-ExternalAgent
Disallow: /

User-agent: Meta-ExternalFetcher
Disallow: /

User-agent: FacebookBot
Disallow: /

User-agent: Applebot-Extended
Disallow: /

User-agent: Bytespider
Disallow: /

# Common scrapers and bad actors
User-agent: SemrushBot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: SiteAuditBot
Disallow: /

User-agent: MegaIndex
Disallow: /

User-agent: SearchmetricsBot
Disallow: /

User-agent: Sogou
Disallow: /

User-agent: 360Spider
Disallow: /

User-agent: ia_archiver
Disallow: /

User-agent: Wayback
Disallow: /

User-agent: archive.org_bot
Disallow: /

User-agent: wget
Disallow: /

User-agent: curl
Disallow: /

User-agent: python-requests
Disallow: /

User-agent: Python-urllib
Disallow: /

User-agent: libwww-perl
Disallow: /

User-agent: HeadlessChrome
Disallow: /

User-agent: PhantomJS
Disallow: /

User-agent: Selenium
Disallow: /

# Content scrapers
User-agent: Nutch
Disallow: /

User-agent: Apache-HttpClient
Disallow: /

User-agent: Go-http-client
Disallow: /

User-agent: Java
Disallow: /

User-agent: Jakarta Commons-HttpClient
Disallow: /

User-agent: Node.js
Disallow: /

User-agent: okhttp
Disallow: /

User-agent: PostmanRuntime
Disallow: /

User-agent: Insomnia
Disallow: /

# Monitoring and uptime checkers
User-agent: UptimeRobot
Disallow: /

User-agent: Pingdom
Disallow: /

User-agent: Site24x7
Disallow: /

User-agent: StatusCake
Disallow: /

User-agent: Uptimia
Disallow: /

# Image and media scrapers
User-agent: ImageSift
Disallow: /

User-agent: PicScout
Disallow: /

User-agent: TinEye
Disallow: /

User-agent: ImagesiftBot
Disallow: /

# Generic patterns that catch programmatic access
User-agent: *bot*
Disallow: /

User-agent: *crawler*
Disallow: /

User-agent: *spider*
Disallow: /

User-agent: *scraper*
Disallow: /

User-agent: *fetch*
Disallow: /

User-agent: *http*
Disallow: /

# Block common headless browsers
User-agent: *headless*
Disallow: /

User-agent: *phantom*
Disallow: /

User-agent: *selenium*
Disallow: /

User-agent: *chrome*
Disallow: /

# No sitemap provided - discourage further crawling